{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Models for Text\n",
    "-------\n",
    "\n",
    "Now, we use the Keras `Tokenizer` to preprocess our spam data and feed it through different architectures of sequential network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('data/sms_spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hope you are having a good week. Just checking in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>K..give back my thanks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary 4 STAR Ibiza Holiday or £10,000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Hope you are having a good week. Just checking in\n",
       "1   ham                            K..give back my thanks.\n",
       "2   ham        Am also doing in cbe only. But have to pay.\n",
       "3  spam  complimentary 4 STAR Ibiza Holiday or £10,000 ...\n",
       "4  spam  okmail: Dear Dave this is your final notice to..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam['text']\n",
    "y = np.where(spam['type'] == 'ham', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tokenizer`\n",
    "------\n",
    "Here, we set the limit to the number of words at 500, then fit the texts, and finally transform our text to sequences of integer values with the `.texts_to_sequences`.  To assure the same length we use the `pad_sequences` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tokenizer and specify the vocabulary\n",
    "tokenizer = Tokenizer(num_words = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit it on text\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate sequences\n",
    "X_vect = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[122, 3, 22, 313, 4, 53, 110, 37, 8], [92, 134, 86, 11, 170]]\n"
     ]
    }
   ],
   "source": [
    "print(X_vect[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences to 100\n",
    "X_seq = pad_sequences(X_vect, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       122,   3,  22, 313,   4,  53, 110,  37,   8], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a peek\n",
    "X_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Networks in 1D\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5559, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5559, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5559, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reshape(-1, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.3411 - accuracy: 0.8858\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2618 - accuracy: 0.9194\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2489 - accuracy: 0.9244\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2404 - accuracy: 0.9255\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2356 - accuracy: 0.9272\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 2s 9ms/step - loss: 0.2318 - accuracy: 0.9282\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 2s 9ms/step - loss: 0.2290 - accuracy: 0.9294\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 2s 9ms/step - loss: 0.2249 - accuracy: 0.9304\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 2s 9ms/step - loss: 0.2232 - accuracy: 0.9312\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 2s 9ms/step - loss: 0.2198 - accuracy: 0.9320\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2171 - accuracy: 0.9328\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2151 - accuracy: 0.9336\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 2s 9ms/step - loss: 0.2117 - accuracy: 0.9345\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2088 - accuracy: 0.9351\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2076 - accuracy: 0.9357\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2064 - accuracy: 0.9360\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 2s 9ms/step - loss: 0.2048 - accuracy: 0.9362\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2037 - accuracy: 0.9370\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2025 - accuracy: 0.9377\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 2s 10ms/step - loss: 0.2011 - accuracy: 0.9379\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim = tokenizer.num_words, output_dim = 64))#creates embeddings\n",
    "\n",
    "model.add(Conv1D(64, 8, activation = 'relu'))\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(Conv1D(64, 16, activation = 'relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'bce', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_seq, y, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
