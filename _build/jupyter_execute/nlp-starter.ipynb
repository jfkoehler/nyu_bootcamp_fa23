{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# NLP I: `CountVectorizer`, `TfidfVectorizer`, and Modeling\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "---\n",
    "\n",
    "- Extract features from unstructured text by fitting and transforming with `CountVectorizer` and `TfidfVectorizer`.\n",
    "- Describe how CountVectorizers and TF-IDFVectorizers work.\n",
    "- Understand `stop_words`, `max_features`, `min_df`, `max_df`, and `ngram_range`.\n",
    "- Implement `CountVectorizer` and `TfidfVectorizer` in a spam classification model.\n",
    "- Use `GridSearchCV` and `Pipeline` with `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# imports\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Text Feature Extraction\n",
    "\n",
    "The models we've learned, like linear regression, logistic regression, and k-nearest neighbors, take in an `X` and a `y` variable.\n",
    "- `X` is a matrix/dataframe of real numbers.\n",
    "- `y` is a vector/series of real numbers.\n",
    "\n",
    "Text data (also called natural language data) is not already organized as a matrix or vector of real numbers. We say that this data is **unstructured**.\n",
    "\n",
    "> This lesson will focus on how to transform our unstructured text data into a numeric `X` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification Model\n",
    "\n",
    "One common application of NLP is predicting \"spam\" vs. \"ham,\" or \"spam\" vs. \"not spam.\"\n",
    "\n",
    "Can we predict real vs. promotional texts just based on what is written?\n",
    "\n",
    "> This data set was taken from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data.\n",
    "spam = pd.read_csv('https://raw.githubusercontent.com/jfkoehler/NYU-Bootcamp/master/notebooks/module_2/2.09_intro-to-nlp/data/sms.csv',\n",
    "                  index_col = 0)\n",
    "\n",
    "# Check out first five rows.\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the size of our data set?\n",
    "spam.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic terminology\n",
    "\n",
    "---\n",
    "\n",
    "- A collection of text is a **document**. \n",
    "    - You can think of a document as a row in your feature matrix.\n",
    "- A collection of documents is a **corpus**. \n",
    "    - You can think of your full dataframe as the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get our data\n",
    "---\n",
    "\n",
    "Convert ham/spam into binary labels:\n",
    "- 0 for ham\n",
    "- 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create label column\n",
    "y = np.where(spam['class'] == 'ham', 0, 1)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our data for modeling:\n",
    "- `X` will be the `message` column. **NOTE**: `CountVectorizer` requires a vector, so make sure you set `X` to be a `pandas` Series, **not** a DataFrame.\n",
    "- `y` will be the `label` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865985\n",
       "spam    0.134015\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what we need to check in a classification problem.\n",
    "# This is the baseline --> accuracy\n",
    "spam['class'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "Let's review some of the pre-processing steps for text data:\n",
    "\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal\n",
    "\n",
    "`CountVectorizer` actually can do a lot of this for us! It is important to keep these steps in mind in case you want to change the default methods used for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `CountVectorizer`\n",
    "---\n",
    "\n",
    "The easiest way for us to convert text data into a structured, numeric `X` dataframe is to use `CountVectorizer`.\n",
    "\n",
    "- **Count**: Count up how many times a token is observed in a given document.\n",
    "- **Vectorizer**: Create a column (also known as a vector) that stores those counts.\n",
    "\n",
    "![](./images/countvectorizer2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer.\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on our corpus.\n",
    "cvec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the corpus.\n",
    "X_train_cvec = cvec.transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/countvectorizer.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does X_train look like now?\n",
    "# print(X_train_cvec[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the vocabulary\n",
    "# cvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '000pes', ..., 'èn', 'ú1', '〨ud'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the feature names\n",
    "cvec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>zed</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4180 rows × 7463 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  000pes  008704050406  0089  0121  01223585236  01223585334  \\\n",
       "0      0    0       0             0     0     0            0            0   \n",
       "1      0    0       0             0     0     0            0            0   \n",
       "2      0    0       0             0     0     0            0            0   \n",
       "3      0    0       0             0     0     0            0            0   \n",
       "4      0    0       0             0     0     0            0            0   \n",
       "...   ..  ...     ...           ...   ...   ...          ...          ...   \n",
       "4175   0    0       0             0     0     0            0            0   \n",
       "4176   0    0       0             0     0     0            0            0   \n",
       "4177   0    0       0             0     0     0            0            0   \n",
       "4178   0    0       0             0     0     0            0            0   \n",
       "4179   0    0       0             0     0     0            0            0   \n",
       "\n",
       "      0125698789  02  ...  zed  zeros  zhong  zindgi  zoe  zogtorius  zyada  \\\n",
       "0              0   0  ...    0      0      0       0    0          0      0   \n",
       "1              0   0  ...    0      0      0       0    0          0      0   \n",
       "2              0   0  ...    0      0      0       0    0          0      0   \n",
       "3              0   0  ...    0      0      0       0    0          0      0   \n",
       "4              0   0  ...    0      0      0       0    0          0      0   \n",
       "...          ...  ..  ...  ...    ...    ...     ...  ...        ...    ...   \n",
       "4175           0   0  ...    0      0      0       0    0          0      0   \n",
       "4176           0   0  ...    0      0      0       0    0          0      0   \n",
       "4177           0   0  ...    0      0      0       0    0          0      0   \n",
       "4178           0   0  ...    0      0      0       0    0          0      0   \n",
       "4179           0   0  ...    0      0      0       0    0          0      0   \n",
       "\n",
       "      èn  ú1  〨ud  \n",
       "0      0   0    0  \n",
       "1      0   0    0  \n",
       "2      0   0    0  \n",
       "3      0   0    0  \n",
       "4      0   0    0  \n",
       "...   ..  ..  ...  \n",
       "4175   0   0    0  \n",
       "4176   0   0    0  \n",
       "4177   0   0    0  \n",
       "4178   0   0    0  \n",
       "4179   0   0    0  \n",
       "\n",
       "[4180 rows x 7463 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform test\n",
    "pd.DataFrame(X_train_cvec.todense(), columns = cvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When we have unstructured text data, there is a lot of information in that text data.\n",
    "- When we force unstructured text data to follow a \"spreadsheet\" or \"dataframe\" structure, we might lose some of that information.\n",
    "- For example, CountVectorizer creates a vector (column) for each token and counts up the number of occurrences of each token in each document.\n",
    "\n",
    "Our tokens are now stored as a **bag-of-words**. This is a simplified way of looking at and storing our data. \n",
    "- Bag-of-words representations discard grammar, order, and structure in the text but track occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we could fit a model (like a logistic regression model or $k$-nearest neighbors model) using our transformed data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's examine some of the different hyperparameters of `CountVectorizer`:\n",
    "- `stop_words`\n",
    "- `max_features`, `max_df`, `min_df`\n",
    "- `ngram_range`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "---\n",
    "\n",
    "Some words are so common that they may not provide legitimate information about the $Y$ variable we're trying to predict.\n",
    "\n",
    "Let's see what our top-occurring words are right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert X_train into a DataFrame.\n",
    "dtm = pd.DataFrame(X_train_cvec.todense(), columns = cvec.get_feature_names_out())\n",
    "\n",
    "# plot top occuring words\n",
    "dtm.sum().sort_values(ascending = False).head(10).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` gives you the option to eliminate stopwords from your corpus when instantiating your vectorizer.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "```\n",
    "\n",
    "You can optionally pass your own list of stopwords that you'd like to remove.\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words=['list', 'of', 'words', 'to', 'stop'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ur          303\n",
       "just        293\n",
       "gt          230\n",
       "lt          230\n",
       "free        221\n",
       "           ... \n",
       "main          1\n",
       "bffs          1\n",
       "bevies        1\n",
       "beverage      1\n",
       "〨ud           1\n",
       "Length: 7198, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "X_train_cvec2 = cvec.fit_transform(X_train)\n",
    "pd.DataFrame(X_train_cvec2.todense(), columns=cvec.get_feature_names_out()).sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary size\n",
    "\n",
    "---\n",
    "One downside to `CountVectorizer` is the size of its vocabulary (`cvec.get_feature_names()`) can get really large. We're creating one column for every unique token in your corpus of data!\n",
    "\n",
    "There are three hyperparameters to help you control this.\n",
    "\n",
    "1. You can set `max_features` to only include the $N$ most popular vocabulary words in the corpus.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_features=1_000) # Only the top 1,000 words from the entire corpus will be saved\n",
    "```\n",
    "\n",
    "2. You can tell `CountVectorizer` to only consider words that occur in **at least** some number of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(min_df=2) # A word must occur in at least two documents from the corpus\n",
    "```\n",
    "\n",
    "3. Conversely, you can tell `CountVectorizer` to only consider words that occur in **at most** some percentage of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_df=.98) # Ignore words that occur in > 98% of the documents from the corpus\n",
    "```\n",
    "\n",
    "Both `max_df` and `min_df` can accept either an integer or a float.\n",
    "- An integer tells us the number of documents.\n",
    "- A float tells us the percentage of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4180x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 38408 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Range\n",
    "---\n",
    "\n",
    "`CountVectorizer` has the ability to capture $n$-word phrases, also called $n$-grams. Consider the following:\n",
    "\n",
    "> The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "In the example sentence, the 2-grams are:\n",
    "- 'the quick'\n",
    "- 'quick brown'\n",
    "- 'brown fox'\n",
    "- 'fox jumped'\n",
    "- 'jumped over'\n",
    "- 'over the'\n",
    "- 'the lazy'\n",
    "- 'lazy dog'\n",
    "\n",
    "The `ngram_range` determines what $n$-grams should be considered as features.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(ngram_range= (1,2)) # Captures every 1-gram and every 2-gram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many 3-grams would be generated from the phrase \"the quick brown fox jumped over the lazy dog?\"</summary>\n",
    "\n",
    "- Seven 3-grams.\n",
    "    - 'the quick brown'\n",
    "    - 'quick brown fox'\n",
    "    - 'brown fox jumped'\n",
    "    - 'fox jumped over'\n",
    "    - 'jumped over the'\n",
    "    - 'over the lazy'\n",
    "    - 'the lazy dog'\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why might we want to change ngram_range to something other than (1,1)?</summary>\n",
    "\n",
    "- We can work with multi-word phrases like \"not good\" or \"very hot.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "---\n",
    "\n",
    "We may want to test lots of different values of hyperparameters in our CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline accuracy\n",
    "\n",
    "We need to calculate baseline accuracy in order to tell if our model is better than null model (predicting the plurality class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.866095\n",
       "1    0.133905\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guessing the majority class every time\n",
    "pd.Series(y_train).value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridSearchCV`\n",
    "---\n",
    "\n",
    "At this point, you could use your `pipeline` object as a model:\n",
    "\n",
    "```python\n",
    "# Estimate how your model will perform on unseen data\n",
    "cross_val_score(pipe, X_train, y_train, cv=3).mean() \n",
    "\n",
    "# Fit your model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "pipe.score(X_train, y_train)\n",
    "\n",
    "# Test score\n",
    "pipe.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "Since we want to tune over the `CountVectorizer`, we'll load our `pipeline` object into `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                ('model', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# Minimum number of documents needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "params = {'vectorizer__max_features': [2000, 3000, 4000, 5000],\n",
    "         'vectorizer__min_df': [2, 3],\n",
    "         'vectorizer__max_df': [.9, .95]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "grid = GridSearchCV(pipe, param_grid=params, cv = 5)\n",
    "    # what object are we optimizing?\n",
    "    # what parameters values are we searching?\n",
    "    # 5-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many models are we fitting here?</summary>\n",
    "\n",
    "- 4 max_features\n",
    "- 2 min_df\n",
    "- 2 max_df\n",
    "- 2 ngram_range\n",
    "- 5-fold CV\n",
    "- 4 * 2 * 2 * 2 * 5 = 160 models\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
       "                                       (&#x27;model&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;vectorizer__max_df&#x27;: [0.9, 0.95],\n",
       "                         &#x27;vectorizer__max_features&#x27;: [2000, 3000, 4000, 5000],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [2, 3]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
       "                                       (&#x27;model&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;vectorizer__max_df&#x27;: [0.9, 0.95],\n",
       "                         &#x27;vectorizer__max_features&#x27;: [2000, 3000, 4000, 5000],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [2, 3]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;model&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                       ('model', LogisticRegression())]),\n",
       "             param_grid={'vectorizer__max_df': [0.9, 0.95],\n",
       "                         'vectorizer__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'vectorizer__min_df': [2, 3]})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9959828602035351"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the best score?\n",
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820652173913044"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer__max_df': 0.9,\n",
       " 'vectorizer__max_features': 2000,\n",
       " 'vectorizer__min_df': 3}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the best hyperparameters?\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "test_preds = grid.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fa2af510550>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate the specificity (TRUE NEG RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Congratulations! We've used `CountVectorizer` to transform our text data into something we can pass into a model.\n",
    "\n",
    "But what if we want to do something more than just count up the occurrence of each token?\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF) Vectorizer\n",
    "\n",
    "---\n",
    "\n",
    "When modeling, which word do you think tends to be the most helpful?\n",
    "- Words that are common across all documents.\n",
    "- Words that are rare across all documents.\n",
    "- Words that are rare across some documents, and common across some documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "\n",
    "- Words that are common in certain documents but rare in other documents tend to be more informative than words that are common in all documents or rare in all documents.\n",
    "- Example: If we were examining poetry over time, the word \"thine\" might be common in some documents but rare in most documents. The word \"thine\" is probably pretty informative in this case.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a score that tells us which words are important to one document, relative to all other documents. Words that occur often in one document but don't occur in many documents contain more predictive power.\n",
    "\n",
    "Variations of the TF-IDF score are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "- If you want to see how it can be calculated, check out [the Wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and [`sklearn`](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) page.\n",
    "\n",
    "<img src=\"./images/tfidfvectorizer.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice Using the `TfidfVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "`sklearn` provides a TF-IDF vectorizer that works similarly to the CountVectorizer.\n",
    "- The arguments `stop_words`, `max_features`, `min_df`, `max_df`, and `ngram_range` also work here.\n",
    "\n",
    "As you did above, instantiate the default `TfidfVectorizer`, then fit the spam and ham data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# convert training data to dataframe\n",
    "\n",
    "\n",
    "# plot top occuring words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Using the `TfidfVectorizer`\n",
    "\n",
    "Let's set up a pipeline using tf-idf and Multinomial Naive Bayes.\n",
    "\n",
    "<details><summary>What's the problem with this?</summary>\n",
    "\n",
    "- Technically, we are supposed to have positive integers to use Multinomial Naive Bayes. Tf-idf does not give us positive integers.\n",
    "- However, it will still work. Even the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) says \"The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. tf-idf vectorizer (transformer)\n",
    "# 2. LogisticRegression (estimator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "\n",
    "\n",
    "# Save confusion matrix values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXIT QUESTIONS**: [link](https://forms.gle/ZSzHy544KG45J3F7A)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}